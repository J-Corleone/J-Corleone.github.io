{"meta":{"title":"Corleone","subtitle":"Charon_","description":"","author":"Corleone","url":"http://J-Corleone.github.io","root":"/"},"pages":[{"title":"404","date":"2022-05-24T20:36:26.000Z","updated":"2022-05-25T03:36:55.994Z","comments":true,"path":"404/index.html","permalink":"http://j-corleone.github.io/404/index.html","excerpt":"","text":""},{"title":"contact","date":"2022-05-24T20:32:11.000Z","updated":"2022-05-25T06:47:21.557Z","comments":true,"path":"contact/index.html","permalink":"http://j-corleone.github.io/contact/index.html","excerpt":"","text":"本功能暂未开通你可以自言自语🤪"},{"title":"about","date":"2022-05-24T20:29:56.000Z","updated":"2022-05-25T07:19:24.322Z","comments":true,"path":"about/index.html","permalink":"http://j-corleone.github.io/about/index.html","excerpt":"","text":"这里有东西吗"},{"title":"categories","date":"2022-05-24T20:26:33.000Z","updated":"2022-05-24T20:31:50.794Z","comments":true,"path":"categories/index.html","permalink":"http://j-corleone.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2022-05-24T20:28:28.000Z","updated":"2022-05-24T20:31:18.183Z","comments":true,"path":"tags/index.html","permalink":"http://j-corleone.github.io/tags/index.html","excerpt":"","text":""},{"title":"friends","date":"2022-05-24T20:33:19.000Z","updated":"2022-05-24T20:33:44.509Z","comments":true,"path":"friends/index.html","permalink":"http://j-corleone.github.io/friends/index.html","excerpt":"","text":""}],"posts":[{"title":"《Spark HA & Yarn配置》","slug":"Spark3","date":"2022-05-31T05:41:17.000Z","updated":"2022-05-31T05:43:38.486Z","comments":true,"path":"2022/05/31/spark3/","link":"","permalink":"http://j-corleone.github.io/2022/05/31/spark3/","excerpt":"","text":"Spark-Standalone-HA模式Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。此处因为先前配置时的 zookeeper 版本和 spark 版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的 zookeeper配置之前需要删除三台主机的 旧版 zookeeper 以及 对应的软连接在 master 节点上重新进行前面配置的 zookeeper 操作 1.上传apache-zookeeper-3.7.0-bin.tar.gz 到/export/server/目录下 并解压文件 2.在 /export/server 目录下创建软连接 3.进入 /export/server/zookeeper/conf/ 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 4.接上步给 zoo.cfg 添加内容 5.进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 1 写入进 去6.将 master 节点中 /export/server/zookeeper-3.7.0 路径下内容推送给slave1 和 slave2 7.推送成功后，分别在 slave1 和 slave2 上创建软连接 8.接上步推送完成后将 slave1 和 slave2 的 /export/server/zookeeper/zkdatas/文件夹 下的 myid 中的内容分别改为 2 和 3 配置环境变量： 因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此 处也是创建软连接的方便之处 进入 /export/server/spark/conf 文件夹 修改 spark-env.sh 文件内容 cd /export/server/spark/confvim spark-env.sh 为 83 行内容加上注释，此部分原为指定 某台主机 做 master ，加上注释后即为 任何主机都可以做 master 结果显示：……82 # 告知Spark的master运行在哪个机器上 83 # export SPARK_MASTER_HOST=master……… 文末添加内容 SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir=/spark-ha”#spark.deploy.recoveryMode指定HA模式 基于Zookeeper实现#指定Zookeeper的连接地址#指定在Zookeeper中注册临时节点的路径 分发 spark-env.sh 到 salve1 和 slave2 上 scp spark-env.sh slave1:/export/server/spark/conf/ scp spark-env.sh slave2:/export/server/spark/conf/ 启动之前确保 Zookeeper 和 HDFS 均已经启动启动集群: #在 master 上 启动一个master 和全部worker /export/server/spark/sbin/start-all.sh # 注意, 下面命令在 slave1 上执行 启动 slave1 上的 master 做备用 master /export/server/spark/sbin/start-master.sh结果显示：(base) [root@master ~]# jps37328 DataNode41589 Master35798 QuorumPeerMain38521 ResourceManager46281 Jps38907 NodeManager41821 Worker36958 NameNode (base)[root@slave1 sbin]# jps36631 DataNode48135 Master35385 QuorumPeerMain37961 NodeManager40970 Worker48282 Jps37276 SecondaryNameNode 访问 WebUI 界面 http://master:8081/ http://slave1:8082/ 此时 kill 掉 master 上的 master 假设 master 主机宕机掉 #master主机 master 的进程号 kill -9 41589 结果显示： (base) [root@master ~]# jps 37328 DataNode 90336 Jps 35798 QuorumPeerMain 38521 ResourceManager 38907 NodeManager 41821 Worker 36958 NameNode 访问 slave1 的 WebUI http://slave1:8082/ 进行主备切换的测试提交一个 spark 任务到当前 活跃的 master上 : /export/server/spark/bin/spark-submit –master spark://master:7077 /export/server/spark/examples/src/main/python/pi.py 1000 复制标签 kill 掉 master 的 进程号再次访问 master 的 WebUI http://master:8081/网页访问不了！ 再次访问 slave1 的 WebUI http://slave1:8082/ 可以看到当前活跃的 master 提示信息 (base) [root@master ~]# /export/server/spark/bin/spark-submit –master spark://master:7077 /export/server/spark/examples/src/main/python/pi.py 1000 22/03/29 16:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 22/03/29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… 22/03/29 16:12:16 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection… 22/03/29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… Pi is roughly 3.140960 (base) [root@master ~]# Spark On YARN模式 在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无 需部署Spark集群, 只要找一台服务器, 充当Spark的客户端保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在 spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了） spark-env.sh 文件部分显示： …. 77 ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 78 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop 79 YARN_CONF_DIR=/export/server/hadoop/etc/hadoop …. 链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式）bin/pyspark –master yarn –deploy-mode client|cluster # –deploy-mode 选项是指定部署模式, 默认是 客户端模式 # client就是客户端模式 # cluster就是集群模式 # –deploy-mode 仅可以用在YARN模式下bin/spark-shell –master yarn –deploy-mode client|clusterbin/spark-submit –master yarn –deploy-mode client|cluster /xxx/xxx/xxx.py 参数 spark-submit 和 spark-shell 和 pyspark的相关参数 bin/pyspark: pyspark解释器spark环境 - bin/spark-shell: scala解释器spark环境 - bin/spark-submit: 提交jar包或Python文件执行的工具 - bin/spark-sql: sparksql客户端工具 这4个客户端工具的参数基本通用.以spark-submit 为例: bin/spark-submit –master spark://master:7077 xxx.py` Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]Usage: spark-submit –kill [submission ID] –master [spark://…]Usage: spark-submit –status [submission ID] –master [spark://…]Usage: spark-submit run-example [options] example-class [example args] Options: –master MASTER_URL spark://host:port, mesos://host:port, yarn, k8s://https://host:port, or local (Default: local[*]). –deploy-mode DEPLOY_MODE 部署模式 client 或者 cluster 默认是client –class CLASS_NAME 运行java或者scala class(for Java / Scala apps). –name NAME 程序的名字 –jars JARS Comma-separated list of jars to include on the driver and executor classpaths. –packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by –repositories. The format for the coordinates should be groupId:artifactId:version.–exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in– packages to avoid dependency conflicts. –repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with– packages.–py-files PY_FILES 指定Python程序依赖的其它python文件–files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName).–archives ARCHIVES Comma-separated list of archives to be extracted into the working directory of each executor.–conf,-c PROP=VALUE 手动指定配置–properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark- defaults.conf. –driver-memory MEM Driver的可用内存(Default: 1024M). –driver-java-options Driver的一些Java选项 –driver-library-path Extra library path entries to pass to the driver. –driver-class-path Extra class path entries to pass to the driver. Note that jars added with –jars are automatically included in the classpath.–executor-memory MEM Executor的内存 (Default: 1G).–proxy-user NAME User to impersonate when submitting the application. This argument does not work with–principal /–keytab.–help,-h 显示帮助文件–verbose,-v Print additional debug output. –version, 打印版本 Cluster deploy mode only(集群模式专属):–driver-cores NUM Driver可用的的CPU核数(Default: 1). Spark standalone or Mesos with cluster deploy mode only:–supervise 如果给定, 可以尝试重启Driver Spark standalone, Mesos or K8s with cluster deploy mode only:–kill SUBMISSION_ID 指定程序ID kill –status SUBMISSION_ID 指定程序ID 查看运行状态 Spark standalone, Mesos and Kubernetes only:–total-executor-cores NUM 整个任务可以给Executor多少个CPU核心用 Spark standalone, YARN and Kubernetes only:–executor-cores NUM 单个Executor能使用多少CPU核心 Spark on YARN and Kubernetes only(YARN模式下):–num-executors NUM Executor应该开启几个–principal PRINCIPAL Principal to be used to login to KDC.–keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. Spark on YARN only:–queue QUEUE_NAME 指定运行的YARN队列(Default: “default”) 启动 YARN 的历史服务器 cd /export/server/hadoop-3.3.0/sbin ./mr-jobhistory-daemon.sh start historyserver 访问WebUI界面 http://master:19888/ client 模式测试 SPARK_HOME=/export/server/spark ${SPARK_HOME}/bin/spark-submit –master yarn –deploy-mode client – driver-memory 512m –executor-memory 512m –num-executors 1 –total- executor-cores 2 ${SPARK_HOME}/examples/src/main/python/pi.py cluster 模式测试 SPARK_HOME=/export/server/spark ${SPARK_HOME}/bin/spark-submit –master yarn –deploy-mode cluster –driver- memory 512m –executor-memory 512m –num-executors 1 –total-executor-cores 2 –conf “spark.pyspark.driver.python=/root/anaconda3/bin/python3” –conf “spark.pyspark.python=/root/anaconda3/bin/python3” ${SPARK_HOME}/examples/src/main/python/pi.py 3**","categories":[],"tags":[]},{"title":"《Spark local& stand-alone配置》","slug":"Spark2","date":"2022-05-31T05:36:26.000Z","updated":"2022-05-31T05:41:06.167Z","comments":true,"path":"2022/05/31/spark2/","link":"","permalink":"http://j-corleone.github.io/2022/05/31/spark2/","excerpt":"","text":"Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。此处因为先前配置时的 zookeeper 版本和 spark 版本不太兼容，导致此模式有故障，需要重新下载配置新的版本的 zookeeper配置之前需要删除三台主机的 旧版 zookeeper 以及 对应的软连接在 master 节点上重新进行前面配置的 zookeeper 操作 1.上传apache-zookeeper-3.7.0-bin.tar.gz 到/export/server/目录下 并解压文件 2.在 /export/server 目录下创建软连接 3.进入 /export/server/zookeeper/conf/ 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 4.接上步给 zoo.cfg 添加内容 5.进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 1 写入进 去6.将 master 节点中 /export/server/zookeeper-3.7.0 路径下内容推送给slave1 和 slave2 7.推送成功后，分别在 slave1 和 slave2 上创建软连接 8.接上步推送完成后将 slave1 和 slave2 的 /export/server/zookeeper/zkdatas/文件夹 下的 myid 中的内容分别改为 2 和 3 配置环境变量： 因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此 处也是创建软连接的方便之处 进入 /export/server/spark/conf 文件夹 修改 spark-env.sh 文件内容 cd /export/server/spark/confvim spark-env.sh 为 83 行内容加上注释，此部分原为指定 某台主机 做 master ，加上注释后即为 任何主机都可以做 master 结果显示：……82 # 告知Spark的master运行在哪个机器上 83 # export SPARK_MASTER_HOST=master……… 文末添加内容 SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir=/spark-ha”#spark.deploy.recoveryMode指定HA模式 基于Zookeeper实现#指定Zookeeper的连接地址#指定在Zookeeper中注册临时节点的路径 分发 spark-env.sh 到 salve1 和 slave2 上 scp spark-env.sh slave1:/export/server/spark/conf/ scp spark-env.sh slave2:/export/server/spark/conf/ 启动之前确保 Zookeeper 和 HDFS 均已经启动启动集群: #在 master 上 启动一个master 和全部worker /export/server/spark/sbin/start-all.sh # 注意, 下面命令在 slave1 上执行 启动 slave1 上的 master 做备用 master /export/server/spark/sbin/start-master.sh结果显示：(base) [root@master ~]# jps37328 DataNode41589 Master35798 QuorumPeerMain38521 ResourceManager46281 Jps38907 NodeManager41821 Worker36958 NameNode (base)[root@slave1 sbin]# jps36631 DataNode48135 Master35385 QuorumPeerMain37961 NodeManager40970 Worker48282 Jps37276 SecondaryNameNode 访问 WebUI 界面 http://master:8081/ http://slave1:8082/ 此时 kill 掉 master 上的 master 假设 master 主机宕机掉 #master主机 master 的进程号 kill -9 41589 结果显示： (base) [root@master ~]# jps 37328 DataNode 90336 Jps 35798 QuorumPeerMain 38521 ResourceManager 38907 NodeManager 41821 Worker 36958 NameNode 访问 slave1 的 WebUI http://slave1:8082/ 进行主备切换的测试提交一个 spark 任务到当前 活跃的 master上 : /export/server/spark/bin/spark-submit –master spark://master:7077 /export/server/spark/examples/src/main/python/pi.py 1000 复制标签 kill 掉 master 的 进程号再次访问 master 的 WebUI http://master:8081/网页访问不了！ 再次访问 slave1 的 WebUI http://slave1:8082/ 可以看到当前活跃的 master 提示信息 (base) [root@master ~]# /export/server/spark/bin/spark-submit –master spark://master:7077 /export/server/spark/examples/src/main/python/pi.py 1000 22/03/29 16:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 22/03/29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… 22/03/29 16:12:16 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection… 22/03/29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… Pi is roughly 3.140960 (base) [root@master ~]# Spark On YARN模式 在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无 需部署Spark集群, 只要找一台服务器, 充当Spark的客户端保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在 spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了） spark-env.sh 文件部分显示： …. 77 ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 78 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop 79 YARN_CONF_DIR=/export/server/hadoop/etc/hadoop …. 链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式）bin/pyspark –master yarn –deploy-mode client|cluster # –deploy-mode 选项是指定部署模式, 默认是 客户端模式 # client就是客户端模式 # cluster就是集群模式 # –deploy-mode 仅可以用在YARN模式下bin/spark-shell –master yarn –deploy-mode client|clusterbin/spark-submit –master yarn –deploy-mode client|cluster /xxx/xxx/xxx.py 参数 spark-submit 和 spark-shell 和 pyspark的相关参数 bin/pyspark: pyspark解释器spark环境 - bin/spark-shell: scala解释器spark环境 - bin/spark-submit: 提交jar包或Python文件执行的工具 - bin/spark-sql: sparksql客户端工具 这4个客户端工具的参数基本通用.以spark-submit 为例: bin/spark-submit –master spark://master:7077 xxx.py` Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]Usage: spark-submit –kill [submission ID] –master [spark://…]Usage: spark-submit –status [submission ID] –master [spark://…]Usage: spark-submit run-example [options] example-class [example args] Options: –master MASTER_URL spark://host:port, mesos://host:port, yarn, k8s://https://host:port, or local (Default: local[*]). –deploy-mode DEPLOY_MODE 部署模式 client 或者 cluster 默认是client –class CLASS_NAME 运行java或者scala class(for Java / Scala apps). –name NAME 程序的名字 –jars JARS Comma-separated list of jars to include on the driver and executor classpaths. –packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by –repositories. The format for the coordinates should be groupId:artifactId:version.–exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in– packages to avoid dependency conflicts. –repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with– packages.–py-files PY_FILES 指定Python程序依赖的其它python文件–files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName).–archives ARCHIVES Comma-separated list of archives to be extracted into the working directory of each executor.–conf,-c PROP=VALUE 手动指定配置–properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark- defaults.conf. –driver-memory MEM Driver的可用内存(Default: 1024M). –driver-java-options Driver的一些Java选项 –driver-library-path Extra library path entries to pass to the driver. –driver-class-path Extra class path entries to pass to the driver. Note that jars added with –jars are automatically included in the classpath.–executor-memory MEM Executor的内存 (Default: 1G).–proxy-user NAME User to impersonate when submitting the application. This argument does not work with–principal /–keytab.–help,-h 显示帮助文件–verbose,-v Print additional debug output. –version, 打印版本 Cluster deploy mode only(集群模式专属):–driver-cores NUM Driver可用的的CPU核数(Default: 1). Spark standalone or Mesos with cluster deploy mode only:–supervise 如果给定, 可以尝试重启Driver Spark standalone, Mesos or K8s with cluster deploy mode only:–kill SUBMISSION_ID 指定程序ID kill –status SUBMISSION_ID 指定程序ID 查看运行状态 Spark standalone, Mesos and Kubernetes only:–total-executor-cores NUM 整个任务可以给Executor多少个CPU核心用 Spark standalone, YARN and Kubernetes only:–executor-cores NUM 单个Executor能使用多少CPU核心 Spark on YARN and Kubernetes only(YARN模式下):–num-executors NUM Executor应该开启几个–principal PRINCIPAL Principal to be used to login to KDC.–keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. Spark on YARN only:–queue QUEUE_NAME 指定运行的YARN队列(Default: “default”) 启动 YARN 的历史服务器 cd /export/server/hadoop-3.3.0/sbin ./mr-jobhistory-daemon.sh start historyserver 访问WebUI界面 http://master:19888/ client 模式测试 SPARK_HOME=/export/server/spark ${SPARK_HOME}/bin/spark-submit –master yarn –deploy-mode client – driver-memory 512m –executor-memory 512m –num-executors 1 –total- executor-cores 2 ${SPARK_HOME}/examples/src/main/python/pi.py cluster 模式测试 SPARK_HOME=/export/server/spark ${SPARK_HOME}/bin/spark-submit –master yarn –deploy-mode cluster –driver- memory 512m –executor-memory 512m –num-executors 1 –total-executor-cores 2 –conf “spark.pyspark.driver.python=/root/anaconda3/bin/python3” –conf “spark.pyspark.python=/root/anaconda3/bin/python3” ${SPARK_HOME}/examples/src/main/python/pi.py 3** 《Spark local&amp; stand-alone配置》 ** 本地模式(单机) 本地模式就是以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境Anaconda On Linux 安装 (单台服务器脚本安装)安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 /export/server: cd /export/server#运行文件 sh Anaconda3-2021.05-Linux-x86_64.sh过程显示：…#出现内容选 yes Please answer ‘yes’ or ‘no’:’ &gt;&gt;&gt; yes …#出现添加路径：/export/server/anaconda3…[/root/anaconda3] /export/server/anaconda3 PREFIX=/export/server/anaconda3… 安装完成后, 退出终端， 重新进来: exit结果显示：#看到这个Base开头表明安装好了.base是默认的虚拟环境. Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1 (base)[root@node1 ~]# 创建虚拟环境 pyspark 基于 python3.8 conda create -n pyspark python=3.8 切换到虚拟环境内 conda activate pyspark 结果显示： (base) [root@node1 ~]# conda activate pyspark (pyspark) [root@node1 ~]# 在虚拟环境内安装包 （有WARNING不用管） pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple spark 安装将文件上传到 /export/server 里面 ，解压 cd /export/server # 解压 tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/ 建立软连接 ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark 添加环境变量 SPARK_HOME: 表示Spark安装路径在哪里PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器JAVA_HOME: 告知Spark Java在哪里HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里HADOOP_HOME: 告知Spark Hadoop安装在哪里 vim /etc/profile内容：…..注：此部分之前配置过，此部分不需要在配置#JAVA_HOME export JAVA_HOME=/export/server/jdk1.8.0_241 export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar #HADOOP_HOME export HADOOP_HOME=/export/server/hadoop-3.3.0 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin #ZOOKEEPER_HOME export ZOOKEEPER_HOME=/export/server/zookeeper export PATH=$PATH:$ZOOKEEPER_HOME/bin ….. #将以下部分添加进去 #SPARK_HOME export SPARK_HOME=/export/server/spark #HADOOP_CONF_DIR export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop #PYSPARK_PYTHON export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/pythonvim .bashrc内容添加进去： #JAVA_HOMEexport JAVA_HOME=/export/server/jdk1.8.0_241#PYSPARK_PYTHONexport PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python 重新加载环境变量文件 source /etc/profilesource ~/.bashrc 进入 /export/server/anaconda3/envs/pyspark/bin/ 文件夹 cd /export/server/anaconda3/envs/pyspark/bin/ 开启 ./pyspark 结果显示： (base) [root@master bin]# ./pysparkPython 3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0] :: Anaconda, Inc. on linuxType “help”, “copyright”, “credits” or “license” for more information.Setting default log level to “WARN”. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 2022-03-15 20:37:04,612 WARN util.NativeCodeLoader: Unable to load native- hadoop library for your platform… using builtin-java classes where applicableWelcome to __ / / ___ ****/ /***\\ / _ / _ `/ */ ‘*//* / .**/,*// //_\\ version 3.2.0//Using Python version 3.8.12 (default, Oct 12 2021 13:49:34) Spark context Web UI available at http://master:4040 Spark context available as ‘sc’ (master = local[], app id = local- 1647347826262). SparkSession available as ‘spark’. &gt;&gt;&gt; 查看WebUI界面 浏览器访问：http://node1:4040/ 退出 conda deactivate Standalone模式(集群) Spark中的各个角色以独立进程的形式存在,并组成Spark集群环境 Anaconda On Linux 安装 (单台服务器脚本安装 注：在 slave1 和 slave2 上部署)安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 /export/server: cd /export/server # 运行文件 sh Anaconda3-2021.05-Linux-x86_64.sh 过程显示：…#出现内容选 yesPlease answer ‘yes’ or ‘no’:’yes…#出现添加路径：/export/server/anaconda3…[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3 PREFIX=/export/server/anaconda3… 安装完成后, 退出终端， 重新进来:exit结果显示：#看到这个Base开头表明安装好了.base是默认的虚拟环境.Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1… 在 master 节点上把 ./bashrc 和 profile 分发给 slave1 和 slave2 #分发 .bashrc : scp /.bashrc root@slave1:/ scp /.bashrc root@slave2:/ #分发 profile : scp /etc/profile/ root@slave1:/etc/ scp /etc/profile/ root@slave2:/etc/… 创建虚拟环境 pyspark 基于 python3.8 conda create -n pyspark python=3.8 切换到虚拟环境内 conda activate pyspark 结果显示： (base) [root@node1 ~]# conda activate pyspark (pyspark)在虚拟环境内安装包 （有WARNING不用管）pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simplespark 安装将文件上传到 /export/server 里面 ，解压 master 节点节点进入 /export/server/spark/conf 修改以下配置文件 cd /export/server/spark/conf 将文件 workers.template 改名为 workers，并配置文件内容 mv workers.template workers vim workers# localhost删除，内容追加文末： node1node2node3# 功能: 这个文件就是指示了 当前SparkStandAlone环境下, 有哪些worker 将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容 mv spark-env.sh.template spark-env.sh vim spark-env.sh 文末追加内容：##设置JAVA安装目录 JAVA_HOME=/export/server/jdk##HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop YARN_CONF_DIR=/export/server/hadoop/etc/hadoop ## 指定spark老大Master的IP和提交任务的通信端口 # 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST=master#告知sparkmaster的通讯端口 export SPARK_MASTER_PORT=7077# 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT=8080# worker cpu可用核数 SPARK_WORKER_CORES=1# worker可用内存 SPARK_WORKER_MEMORY=1g# worker的工作通讯地址 SPARK_WORKER_PORT=7078# worker的 webui地址 SPARK_WORKER_WEBUI_PORT=8081## 设置历史服务器 # 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中 SPARK_HISTORY_OPTS=”- Dspark.history.fs.logDirectory=hdfs://master:8020/sparklog/ - Dspark.history.fs.cleaner.enabled=true” 开启 hadoop 的 hdfs 和 yarn 集群 start-dfs.sh start-yarn.sh 在HDFS上创建程序运行历史记录存放的文件夹，同样 conf 文件目录下: hadoop fs -mkdir /sparklog hadoop fs -chmod 777 /sparklog 将 spark-defaults.conf.template 改为 spark-defaults.conf 并做相关配置 mv spark-defaults.conf.template spark-defaults.conf vim spark-defaults.conf 文末追加内容为： # 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs://master:8020/sparklog/ # 设置spark日志是否启动压缩 spark.eventLog.compress true 配置 log4j.properties 文件 将文件第 19 行的 log4j.rootCategory=INFO, console 改为log4j.rootCategory=WARN, console （即将INFO 改为 WARN 目的：输出日志, 设置级别为WARN 只输出警告和错误日志，INFO 则为输出所有信息，多数为无用信息） mv log4j.properties.template log4j.properties vim log4j.properties 结果显示：…18 # Set everything to be logged to the console19 log4j.rootCategory=WARN, console …. master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上 master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上 在slave1 和 slave2 上做软连接 ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark 重新加载环境变量 source /etc/profile 进入 /export/server/spark/sbin 文件目录下 启动 start-history-server.sh cd /export/server/spark/sbin ./start-history-server.sh 访问 WebUI 界面 浏览器访问： http://master:18080/","categories":[],"tags":[]},{"title":"《spark基础环境配置》","slug":"Spark1","date":"2022-05-31T05:25:52.000Z","updated":"2022-05-31T05:36:01.740Z","comments":true,"path":"2022/05/31/spark1/","link":"","permalink":"http://j-corleone.github.io/2022/05/31/spark1/","excerpt":"","text":"打开一个hosts映射文件,为了保证后续相互关联的虚拟机能够通过主机名进行访问，根据实际需求配置对应的IP和主机名映射，分别将主机名master、slave1、slave2 与IP地址 192.168.88.134、192.168.88.135 和192.168.88.136进行了匹配映射(这里通常要根据实际需要，将要搭建的集群主机都配置主机名和IP映射)。编辑 /etc/hosts 文件 vim /etc/hosts内容修改为（注：三台主机内容一样）localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6192.168.88.135 node1192.168.88.136 node2192.168.88.137 node3 三、集群配置时间同步定义：网络时间服务协议（Network Time Protocol, NTP），是用来使计算机时间同步化的一种协议，它可以使计算机对其服务器做时间同步化。原因：时间同步服务器，顾名思义就是来同步时间的。在集群中同步时间有着十分重要的作用，负载均衡集群或高可用集群如果时间不一致，在服务器之间的数据误差就会很大，寻找数据便会成为一件棘手的事情。若是时间无法同步，那么就算是备份了数据，你也可能无法在正确的时间将正确的数据备份。那损失可就大了。yum 安装 ntp （注：三台主机做同样操作） yum install ntp -y开机自启动ntpsystemctl enable ntpd &amp;&amp; systemctl start ntpd结果显示： [root@master ~]# systemctl enable ntpd &amp;&amp; systemctl start ntpd Created symlink from /etc/systemd/system/multi- user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service. 授权 192.168.88.0-192.168.10.255 网段上的所有机器可以从这台机器上查询和同步时间 #查看ntp配置文件ls -al /etc | grep ‘ntp’#显示内容[root@node1 etc]# ls -al /etc | grep ‘ntp’drwxr-xr-x 3 root root 52 3月 10 18:25 ntp-rw-r–r– 1 root root 2041 3月 10 20:03 ntp.conf#编辑内容添加 restrict 192.168.88.0 mask 255.255.255.0 （注：在17行左右） vim /etc/ntp.conf16 # Hosts on local network are less restricted.17 restrict 192.168.88.0 mask 255.255.255.0 集群在局域网中，不使用其他互联网上的时间 #修改 /etc/ntpd.conf 内容 vim vim /etc/ntp.conf # 将21-24行内容注释掉（注：原来未注释） 21 #server 0.centos.pool.ntp.org iburst 22 #server 1.centos.pool.ntp.org iburst 23 #server 2.centos.pool.ntp.org iburst 24 #server 3.centos.pool.ntp.org iburst # 在25行添加 server masterIP 即为： server 192.168.88.135 node 和 node3 相同操作三台主机同时执行 systemctl enable ntpd &amp;&amp; systemctl start ntpd 查看ntp端口 [root@master etc]# ss -tupln | grep ‘123’ udp UNCONN 0 0 192.168.88.135:123 : users:((“ntpd”,pid=54823,fd=19)) udp UNCONN 0 0 127.0.0.1:123 : users:((“ntpd”,pid=54823,fd=18)) udp UNCONN 0 0 :123 *: users:((“ntpd”,pid=54823,fd=16)) udp UNCONN 0 0 [fe80::2832:5f98:5bc0:e621]%ens33:123 [::]:* users:((“ntpd”,pid=54823,fd=23)) udp UNCONN 0 0 [::1]:123 [::]:* users:((“ntpd”,pid=54823,fd=20)) udp UNCONN 0 0 [::]:123 [::]:* users:((“ntpd”,pid=54823,fd=17)) 配置完成后三台主机都需要重启 shutdown -r 0 三台主机同时执行（注：此过程需要5分钟左右） ntpstat 三、ssh免密钥登陆SSH免密钥登陆可以更加方便的实现不同计算机之间的连接和切换master 生成公钥私钥 (一路回车)ssh-keygen #结果显示：[root@master .ssh]# ssh-keygen Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa):Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: SHA256:QUAgFH5KBc/Erlf1JWSBbKeEepPJqMBqpWbc02/uFj8 root@master The key’s randomart image is:+—[RSA 2048]—-+| .=++oo+.o+. || . . ...o . ||. o.++ *.+ o | |.o ++ B ... | |o.=o.o .S | |.*oo.. . | |+ .. . o | | + E | | =o . | +----[SHA256]-----+ 查看隐藏的 .ssh 文件 la -al .ssh # 结果显示 [root@master ~]# ls -al .ssh/ 总用量 16 drwx------ 2 root root 80 3月 10 21:52 . dr-xr-x---. 4 root root 175 3月 10 21:45 .. -rw------- 1 root root 393 3月 10 21:52 authorized_keys -rw------- 1 root root 1675 3月 10 21:48 id_rsa -rw-r--r-- 1 root root 393 3月 10 21:48 id_rsa.pub -rw-r--r-- 1 root root 366 3月 10 21:54 known_hosts master 配置免密登录到master slave1 slave2 ssh-copy-id masterssh-copy-id slave1ssh-copy-id slave2 四、安装配置 jdk编译环境软件安装目录 mkdir -p /export/server JDK 1.8安装 上传 jdk-8u241-linux-x64.tar.gz到/export/server/目录下 并解压文件 tar -zxvf jdk-8u241-linux-x64.tar.gz 配置环境变量 vim /etc/profile export JAVA_HOME=/export/server/jdk1.8.0_241 export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar 重新加载环境变量文件 source /etc/profile 查看 java 版本号 java -version 结果显示： [root@master jdk1.8.0_241]# java -version java version “1.8.0_241” Java(TM) SE Runtime Environment (build 1.8.0_241-b07) Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode) master 节点将 java 传输到 slave1 和 slave2 scp -r /export/server/jdk1.8.0_241/ root@slave1:/export/server/ scp -r /export/server/jdk1.8.0_241/ root@slave2:/export/server/ 配置 slave1 和 slave2 的 jdk 环境变量（注：和上方 master 的配置方法一样）在 master slave1 和slave2 创建软连接 cd /export/serverln -s jdk1.8.0_241/ jdk 重新加载环境变量文件 source /etc/profile zookeeper安装配置配置主机名和IP的映射关系，修改 /etc/hosts 文件，添加 master.root slave1.root slave2.root vim /etc/hosts #结果显示 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.88.135 master master.root 192.168.88.136 slave1 slave1.root 192.168.88.137 slave2 slave2.root zookeeper安装 上传 zookeeper-3.4.10.tar.gz到/export/server/目录下 并解压文件 cd /export/server/ tar -zxvf zookeeper-3.4.10.tar.gz 在 /export/server 目录下创建软连接 cd /export/server ln -s zookeeper-3.4.10/ zookeeper 进入 /export/server/zookeeper/conf/ 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg cd /export/server/zookeeper/conf/ cp zoo_sample.cfg zoo.cfg 接上步给 zoo.cfg 添加内容 #Zookeeper的数据存放目录 dataDir=/export/server/zookeeper/zkdatas# 保留多少个快照 autopurge.snapRetainCount=3# 日志多少小时清理一次 autopurge.purgeInterval=1# 集群中服务器地址 server.1=master:2888:3888 server.2=slave1:2888:3888 server.3=slave2:2888:3888 进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去 cd /export/server/zookeeper/zkdata touch myid echo ‘1’ &gt; myid 将 master 节点中 /export/server/zookeeper-3.4.10 路径下内容推送给slave1 和 slave2 scp -r /export/server/zookeeper-3.4.10/ slave1:$PWD scp -r /export/server/zookeeper-3.4.10/ slave2:$PWD 推送成功后，分别在 slave1 和 slave2 上创建软连接 ln -s zookeeper-3.4.10/ zookeeper 接上步推送完成后将 slave1 和 slave2 的 /export/server/zookeeper/zkdatas/ 文件夹下的 myid中的内容分别改为 2 和3 cd /export/server/zookeeper/zkdatas/ 结果显示： [root@slave1 zkdatas]# vim myid [root@slave1 zkdatas]# more myid 2[root@slave2 zkdatas]# vim myid [root@slave2 zkdatas]# more myid 3 配置zookeeper的环境变量（注：三台主机都需要配置） vim /etc/profile # zookeeper 环境变量 export ZOOKEEPER_HOME=/export/server/zookeeper export PATH=$PATH:$ZOOKEEPER_HOME/bin 重新加载环境变量文件 source /etc/profile 进入 /export/server/zookeeper-3.4.10/bin 目录下启动 zkServer.sh 脚本 （注：三台都需要做） cd /export/server/zookeeper-3.4.10/bin zkServer.sh start结果显示： [root@master bin]# ./zkServer.sh start ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Starting zookeeper … STARTED zookeeper 的状态 zkServer.sh status 结果显示： [root@master server]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@slave1 server]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@slave2 conf]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader jps 结果显示： [root@master server]# jps 125348 QuorumPeerMain 16311 Jps [root@slave1 server]# jps 126688 QuorumPeerMain 17685 Jps [root@slave2 conf]# jps 126733 QuorumPeerMain 17727 Jps 脚本一键启动 vim zkServer.sh#!/bin/bashif [ $# -eq 0 ] ;thenecho “please input param:start stop”elseif [ $1 = start ] ;thenecho “${1}ing master”ssh master “source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start”for i in {1..2}doecho “${1}ping slave${i}” ssh slave${i} \"source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start\" done fiif [ $1 = stop ];thenecho “${1}ping master “ssh master “source /etc/profile;/export/server/zookeeper/bin/zkServer.sh stop”for i in {1..2}doecho “${1}ping slave${i}” ssh slave${i} “source /etc/profile;/export/server/zookeeper/bin/zkServer.sh stop”donefiif [ $1 = status ];thenecho “${1}ing master”ssh master “source /etc/profile;/export/server/zookeeper/bin/zkServer.sh status”for i in {1..2}doecho “${1}ping slave${i}”ssh slave${i} “source /etc/profile;/export/server/zookeeper/bin/zkServer.sh status”donefifi#将文件放在 /bin 目录下 chmod +x zkServer-all.sh &amp;&amp; zkServer-all.sh Hadoop 安装配置把 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 上传到 /export/server 并解压文件 tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 修改配置文件(进入路径 /export/server/hadoop-3.3.0/etc/hadoop) cd /export/server/hadoop-3.3.0/etc/hadoop hadoop-env.sh #文件最后添加 export JAVA_HOME=/export/server/jdk1.8.0_241 export HDFS_NAMENODE_USER=root export HDFS_DATANODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root export YARN_RESOURCEMANAGER_USER=root export YARN_NODEMANAGER_USER=root core-site.xml &lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 - -&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置Hadoop本地保存数据路径 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置HDFS web UI用户身份 --&gt; &lt;property&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; hadoop.proxyuser.root.hosts * hadoop.proxyuser.root.groups * &lt; !– 文件系统垃圾桶保存时间 –&gt; fs.trash.interval 1440 hdfs-site.xml dfs.namenode.secondary.http-address slave1:9868 mapred-site.xml mapreduce.framework.name yarn mapreduce.jobhistory.address master:10020 mapreduce.jobhistory.webapp.address master:19888 yarn.app.mapreduce.am.env HADOOP_MAPRED_HOME=${HADOOP_HOME} mapreduce.map.env HADOOP_MAPRED_HOME=${HADOOP_HOME} mapreduce.reduce.env HADOOP_MAPRED_HOME=${HADOOP_HOME} yarn-site.xml yarn.resourcemanager.hostname master yarn.nodemanager.aux-services mapreduce_shuffle yarn.nodemanager.pmem-check-enabled false yarn.nodemanager.vmem-check-enabled false yarn.log-aggregation-enable true yarn.log.server.url http://master:19888/jobhistory/logs yarn.log-aggregation.retain-seconds 604800 node1 node2 node3 分发同步hadoop安装包 cd /export/serverscp -r hadoop-3.3.0 root@slave1:$PWDscp -r hadoop-3.3.0 root@slave2:$PWD将hadoop添加到环境变量 vim /etc/profile export HADOOP_HOME=/export/server/hadoop-3.3.0 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 重新加载环境变量文件 source /etc/profile Hadoop集群启动 格式化namenode（只有首次启动需要格式化） hdfs namenode -format 脚本一键启动 [root@master ~]# start-dfs.sh Starting namenodes on [master] 上一次登录：五 3月 11 21:27:24 CST 2022pts/0 上 Starting datanodes 上一次登录：五 3月 11 21:27:32 CST 2022pts/0 上 Starting secondary namenodes [slave1] 上一次登录：五 3月 11 21:27:35 CST 2022pts/0 上 [root@master ~]# start-yarn.sh Starting resourcemanager 上一次登录：五 3月 11 21:27:41 CST 2022pts/0 上 Starting nodemanagers 上一次登录：五 3月 11 21:27:51 CST 2022pts/0 上 启动后 输入 jps 查看 [root@master ~]# jps 127729 NameNode 127937 DataNode 14105 Jps 128812 NodeManager 128591 ResourceManager [root@slave1 hadoop]# jps 121889 NodeManager 121559 SecondaryNameNode 7014 Jps 121369 DataNode [root@slave2 hadoop]# jps 6673 Jps 121543 NodeManager 121098 DataNode WEB页面HDFS集群： http://master:9870/ YARN集群： http://master:9870/","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2022-05-24T14:29:13.025Z","updated":"2022-05-24T14:29:13.025Z","comments":true,"path":"2022/05/24/hello-world/","link":"","permalink":"http://j-corleone.github.io/2022/05/24/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[]}