<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>《Spark HA &amp; Yarn配置》</title>
      <link href="/2022/05/31/spark3/"/>
      <url>/2022/05/31/spark3/</url>
      
        <content type="html"><![CDATA[<p>Spark-Standalone-HA模式<br>Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在<br>着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节<br>点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下<br>master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。<br>此处因为先前配置时的 zookeeper 版本和 spark 版本不太兼容，导致此模式有故障，需要重新下<br>载配置新的版本的 zookeeper<br>配置之前需要删除三台主机的 旧版 zookeeper 以及 对应的软连接<br>在 master 节点上重新进行前面配置的 zookeeper 操作</p><blockquote><p>1.上传apache-zookeeper-3.7.0-bin.tar.gz 到/export/server/目录下 并解压文件 2.在 /export/server 目录下创建软连接 3.进入 /export/server/zookeeper/conf/ 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 4.接上步给 zoo.cfg 添加内容 5.进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 1 写入进 去6.将 master 节点中 /export/server/zookeeper-3.7.0 路径下内容推送给slave1 和 slave2 7.推送成功后，分别在 slave1 和 slave2 上创建软连接 8.接上步推送完成后将 slave1 和 slave2 的 /export/server/zookeeper/zkdatas/文件夹 下的 myid 中的内容分别改为 2 和 3 配置环境变量： 因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此 处也是创建软连接的方便之处</p></blockquote><p>进入 /export/server/spark/conf 文件夹 修改 spark-env.sh 文件内容</p><blockquote><p>cd /export/server/spark/conf<br>vim spark-env.sh</p></blockquote><p>为 83 行内容加上注释，此部分原为指定 某台主机 做 master ，加上注释后即为 任何主机都<br>可以做 master</p><blockquote><p>结果显示：<br>……<br>82 # 告知Spark的master运行在哪个机器上 83 # export SPARK_MASTER_HOST=master<br>………</p></blockquote><p>文末添加内容</p><blockquote><p>SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=ZOOKEEPER -<br>Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir=/spark-ha”<br>#spark.deploy.recoveryMode<br>指定HA模式 基于Zookeeper实现<br>#指定Zookeeper的连接地址<br>#指定在Zookeeper中注册临时节点的路径</p></blockquote><p>分发 spark-env.sh 到 salve1 和 slave2 上</p><blockquote><blockquote><pre class="line-numbers language-none"><code class="language-none">scp spark-env.sh slave1:/export/server/spark/conf/ scp spark-env.sh slave2:/export/server/spark/conf/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></blockquote></blockquote><p>启动之前确保 Zookeeper 和 HDFS 均已经启动<br>启动集群:</p><blockquote><p>#在 master 上 启动一个master 和全部worker /export/server/spark/sbin/start-all.sh # 注意, 下面命令在 slave1 上执行 启动 slave1 上的 master 做备用 master /export/server/spark/sbin/start-master.sh<br>结果显示：<br>(base) [root@master ~]# jps<br>37328 DataNode<br>41589 Master<br>35798 QuorumPeerMain<br>38521 ResourceManager<br>46281 Jps<br>38907 NodeManager<br>41821 Worker<br>36958 NameNode (base)<br>[root@slave1 sbin]# jps<br>36631 DataNode<br>48135 Master<br>35385 QuorumPeerMain<br>37961 NodeManager<br>40970 Worker<br>48282 Jps<br>37276 SecondaryNameNode</p></blockquote><p>访问 WebUI 界面</p><blockquote><blockquote><pre class="line-numbers language-none"><code class="language-none">http://master:8081/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote></blockquote><p>此时 kill 掉 master 上的 master 假设 master 主机宕机掉</p><blockquote><p>#master主机 master 的进程号 kill -9 41589 结果显示： (base) [root@master ~]# jps 37328 DataNode 90336 Jps 35798 QuorumPeerMain 38521 ResourceManager 38907 NodeManager 41821 Worker 36958 NameNode</p></blockquote><p>访问 slave1 的 WebUI</p><blockquote><p><a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote><p>进行主备切换的测试<br>提交一个 spark 任务到当前 活跃的 master上 :</p><blockquote><p>/export/server/spark/bin/spark-submit –master spark://master:7077 /export/server/spark/examples/src/main/python/pi.py 1000</p></blockquote><p>复制标签 kill 掉 master 的 进程号<br>再次访问 master 的 WebUI</p><blockquote><p><a href="http://master:8081/">http://master:8081/</a><br>网页访问不了！</p></blockquote><p>再次访问 slave1 的 WebUI</p><blockquote><p><a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote><p>可以看到当前活跃的 master 提示信息</p><blockquote><p>(base) [root@master ~]# /export/server/spark/bin/spark-submit –master spark://master:7077 /export/server/spark/examples/src/main/python/pi.py 1000 22/03/29 16:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 22/03/29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… 22/03/29 16:12:16 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection… 22/03/29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… Pi is roughly 3.140960 (base) [root@master ~]#</p></blockquote><p>Spark On YARN模式</p><blockquote><p>在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无</p></blockquote><p>需部署Spark集群, 只要找一台服务器, 充当Spark的客户端<br>保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在 spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了）</p><blockquote><p>spark-env.sh 文件部分显示： …. 77 ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 78 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop 79 YARN_CONF_DIR=/export/server/hadoop/etc/hadoop ….</p></blockquote><p>链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式）<br>bin/pyspark –master yarn –deploy-mode client|cluster # –deploy-mode 选项是指定部署模式, 默认是 客户端模式 # client就是客户端模式 # cluster就是集群模式 # –deploy-mode 仅可以用在YARN模式下<br>bin/spark-shell –master yarn –deploy-mode client|cluster<br>bin/spark-submit –master yarn –deploy-mode client|cluster /xxx/xxx/xxx.py 参数</p><blockquote><p>spark-submit 和 spark-shell 和 pyspark的相关参数</p></blockquote><ul><li><p>bin/pyspark: pyspark解释器spark环境 - bin/spark-shell: scala解释器spark环境 - bin/spark-submit: 提交jar包或Python文件执行的工具 - bin/spark-sql: sparksql客户端工具</p><p>这4个客户端工具的参数基本通用.以spark-submit 为例: bin/spark-submit –master spark://master:7077 xxx.py`</p><blockquote><blockquote><p>Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]<br>Usage: spark-submit –kill [submission ID] –master [spark://…]<br>Usage: spark-submit –status [submission ID] –master [spark://…]<br>Usage: spark-submit run-example [options] example-class [example args]</p><p>Options: –master MASTER_URL spark://host:port, mesos://host:port, yarn, k8s://<a href="https://host:port/">https://host:port</a>, or</p><p>local (Default: local[*]). –deploy-mode DEPLOY_MODE 部署模式 client 或者 cluster 默认是client –class CLASS_NAME 运行java或者scala class(for Java / Scala apps). –name NAME 程序的名字 –jars JARS Comma-separated list of jars to include on the</p><p>driver and executor classpaths. –packages Comma-separated list of maven coordinates of</p><p>jars to include on the driver and executor classpaths. Will</p><p>search the local maven repo, then maven central and any</p><p>additional remote repositories given by –repositories. The</p><p>format for the coordinates should be</p><p>groupId:artifactId:version.<br>–exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in<br>– packages to avoid dependency conflicts. –repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with<br>– packages.<br>–py-files PY_FILES 指定Python程序依赖的其它python文件<br>–files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName).<br>–archives ARCHIVES Comma-separated list of archives to be extracted into the working directory of each executor.<br>–conf,<br>-c PROP=VALUE 手动指定配置<br>–properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark- defaults.conf. –driver-memory MEM Driver的可用内存(Default: 1024M). –driver-java-options Driver的一些Java选项 –driver-library-path Extra library path entries to pass to the driver. –driver-class-path Extra class path entries to pass to the driver. Note that jars added with –jars are automatically included in the classpath.<br>–executor-memory MEM Executor的内存 (Default: 1G).<br>–proxy-user NAME User to impersonate when submitting the application. This argument does not work with<br>–principal /<br>–keytab.<br>–help,<br>-h 显示帮助文件<br>–verbose,<br>-v Print additional debug output. –version, 打印版本 Cluster deploy mode only(集群模式专属):<br>–driver-cores NUM Driver可用的的CPU核数(Default: 1). Spark standalone or Mesos with cluster deploy mode only:<br>–supervise 如果给定, 可以尝试重启Driver Spark standalone, Mesos or K8s with cluster deploy mode only:<br>–kill SUBMISSION_ID 指定程序ID kill –status SUBMISSION_ID 指定程序ID 查看运行状态 Spark standalone, Mesos and Kubernetes only:<br>–total-executor-cores NUM 整个任务可以给Executor多少个CPU核心用 Spark standalone, YARN and Kubernetes only:<br>–executor-cores NUM 单个Executor能使用多少CPU核心 Spark on YARN and Kubernetes only(YARN模式下):<br>–num-executors NUM Executor应该开启几个<br>–principal PRINCIPAL Principal to be used to login to KDC.<br>–keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. Spark on YARN only:<br>–queue QUEUE_NAME 指定运行的YARN队列(Default: “default”)</p></blockquote></blockquote></li></ul><p>启动 YARN 的历史服务器</p><blockquote><p>cd /export/server/hadoop-3.3.0/sbin ./mr-jobhistory-daemon.sh start historyserver</p></blockquote><p>访问WebUI界面</p><blockquote><p><a href="http://master:19888/">http://master:19888/</a></p></blockquote><p>client 模式测试</p><blockquote><p>SPARK_HOME=/export/server/spark ${SPARK_HOME}/bin/spark-submit –master yarn –deploy-mode client – driver-memory 512m –executor-memory 512m –num-executors 1 –total- executor-cores 2 ${SPARK_HOME}/examples/src/main/python/pi.py</p></blockquote><p>cluster 模式测试</p><blockquote><p>SPARK_HOME=/export/server/spark ${SPARK_HOME}/bin/spark-submit –master yarn –deploy-mode cluster –driver- memory 512m –executor-memory 512m –num-executors 1 –total-executor-cores 2 –conf “spark.pyspark.driver.python=/root/anaconda3/bin/python3” –conf “spark.pyspark.python=/root/anaconda3/bin/python3” ${SPARK_HOME}/examples/src/main/python/pi.py 3**</p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>《Spark local&amp; stand-alone配置》</title>
      <link href="/2022/05/31/spark2/"/>
      <url>/2022/05/31/spark2/</url>
      
        <content type="html"><![CDATA[<p>Spark Standalone集群是Master-Slaves架构的集群模式,和大部分的Master-Slaves结构集群一样,存在<br>着Master 单点故障(SPOF)的问题。简单理解为，spark-Standalone 模式下为 master 节点控制其他节<br>点，当 master 节点出现故障时，集群就不可用了。 spark-Standalone-HA 模式下<br>master 节点不固定，当一个宕机时，立即换另一台为 master 保障不出现故障。<br>此处因为先前配置时的 zookeeper 版本和 spark 版本不太兼容，导致此模式有故障，需要重新下<br>载配置新的版本的 zookeeper<br>配置之前需要删除三台主机的 旧版 zookeeper 以及 对应的软连接<br>在 master 节点上重新进行前面配置的 zookeeper 操作</p><blockquote><p>1.上传apache-zookeeper-3.7.0-bin.tar.gz 到/export/server/目录下 并解压文件 2.在 /export/server 目录下创建软连接 3.进入 /export/server/zookeeper/conf/ 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg 4.接上步给 zoo.cfg 添加内容 5.进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 1 写入进 去6.将 master 节点中 /export/server/zookeeper-3.7.0 路径下内容推送给slave1 和 slave2 7.推送成功后，分别在 slave1 和 slave2 上创建软连接 8.接上步推送完成后将 slave1 和 slave2 的 /export/server/zookeeper/zkdatas/文件夹 下的 myid 中的内容分别改为 2 和 3 配置环境变量： 因先前配置 zookeeper 时候创建过软连接且以 ’zookeeper‘ 为路径，所以不用配置环境变量，此 处也是创建软连接的方便之处</p></blockquote><p>进入 /export/server/spark/conf 文件夹 修改 spark-env.sh 文件内容</p><blockquote><p>cd /export/server/spark/conf<br>vim spark-env.sh</p></blockquote><p>为 83 行内容加上注释，此部分原为指定 某台主机 做 master ，加上注释后即为 任何主机都<br>可以做 master</p><blockquote><p>结果显示：<br>……<br>82 # 告知Spark的master运行在哪个机器上 83 # export SPARK_MASTER_HOST=master<br>………</p></blockquote><p>文末添加内容</p><blockquote><p>SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=ZOOKEEPER -<br>Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 - Dspark.deploy.zookeeper.dir=/spark-ha”<br>#spark.deploy.recoveryMode<br>指定HA模式 基于Zookeeper实现<br>#指定Zookeeper的连接地址<br>#指定在Zookeeper中注册临时节点的路径</p></blockquote><p>分发 spark-env.sh 到 salve1 和 slave2 上</p><blockquote><blockquote><pre class="line-numbers language-none"><code class="language-none">scp spark-env.sh slave1:/export/server/spark/conf/ scp spark-env.sh slave2:/export/server/spark/conf/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></blockquote></blockquote><p>启动之前确保 Zookeeper 和 HDFS 均已经启动<br>启动集群:</p><blockquote><p>#在 master 上 启动一个master 和全部worker /export/server/spark/sbin/start-all.sh # 注意, 下面命令在 slave1 上执行 启动 slave1 上的 master 做备用 master /export/server/spark/sbin/start-master.sh<br>结果显示：<br>(base) [root@master ~]# jps<br>37328 DataNode<br>41589 Master<br>35798 QuorumPeerMain<br>38521 ResourceManager<br>46281 Jps<br>38907 NodeManager<br>41821 Worker<br>36958 NameNode (base)<br>[root@slave1 sbin]# jps<br>36631 DataNode<br>48135 Master<br>35385 QuorumPeerMain<br>37961 NodeManager<br>40970 Worker<br>48282 Jps<br>37276 SecondaryNameNode</p></blockquote><p>访问 WebUI 界面</p><blockquote><blockquote><pre class="line-numbers language-none"><code class="language-none">http://master:8081/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote></blockquote><p>此时 kill 掉 master 上的 master 假设 master 主机宕机掉</p><blockquote><p>#master主机 master 的进程号 kill -9 41589 结果显示： (base) [root@master ~]# jps 37328 DataNode 90336 Jps 35798 QuorumPeerMain 38521 ResourceManager 38907 NodeManager 41821 Worker 36958 NameNode</p></blockquote><p>访问 slave1 的 WebUI</p><blockquote><p><a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote><p>进行主备切换的测试<br>提交一个 spark 任务到当前 活跃的 master上 :</p><blockquote><p>/export/server/spark/bin/spark-submit –master spark://master:7077 /export/server/spark/examples/src/main/python/pi.py 1000</p></blockquote><p>复制标签 kill 掉 master 的 进程号<br>再次访问 master 的 WebUI</p><blockquote><p><a href="http://master:8081/">http://master:8081/</a><br>网页访问不了！</p></blockquote><p>再次访问 slave1 的 WebUI</p><blockquote><p><a href="http://slave1:8082/">http://slave1:8082/</a></p></blockquote><p>可以看到当前活跃的 master 提示信息</p><blockquote><p>(base) [root@master ~]# /export/server/spark/bin/spark-submit –master spark://master:7077 /export/server/spark/examples/src/main/python/pi.py 1000 22/03/29 16:11:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable 22/03/29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… 22/03/29 16:12:16 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection… 22/03/29 16:12:16 WARN StandaloneAppClient$ClientEndpoint: Connection to master:7077 failed; waiting for master to reconnect… Pi is roughly 3.140960 (base) [root@master ~]#</p></blockquote><p>Spark On YARN模式</p><blockquote><p>在已有YARN集群的前提下在单独准备Spark StandAlone集群,对资源的利用就不高.Spark On YARN, 无</p></blockquote><p>需部署Spark集群, 只要找一台服务器, 充当Spark的客户端<br>保证 HADOOP_CONF_和 DIR_YARN_CONF_DIR 已经配置在 spark-env.sh 和环境变量中 （注: 前面配置spark-Standlone 时已经配置过此项了）</p><blockquote><p>spark-env.sh 文件部分显示： …. 77 ## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 78 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop 79 YARN_CONF_DIR=/export/server/hadoop/etc/hadoop ….</p></blockquote><p>链接到 YARN 中（注: 交互式环境 pyspark 和 spark-shell 无法运行 cluster模式）<br>bin/pyspark –master yarn –deploy-mode client|cluster # –deploy-mode 选项是指定部署模式, 默认是 客户端模式 # client就是客户端模式 # cluster就是集群模式 # –deploy-mode 仅可以用在YARN模式下<br>bin/spark-shell –master yarn –deploy-mode client|cluster<br>bin/spark-submit –master yarn –deploy-mode client|cluster /xxx/xxx/xxx.py 参数</p><blockquote><p>spark-submit 和 spark-shell 和 pyspark的相关参数</p></blockquote><ul><li><p>bin/pyspark: pyspark解释器spark环境 - bin/spark-shell: scala解释器spark环境 - bin/spark-submit: 提交jar包或Python文件执行的工具 - bin/spark-sql: sparksql客户端工具</p><p>这4个客户端工具的参数基本通用.以spark-submit 为例: bin/spark-submit –master spark://master:7077 xxx.py`</p><blockquote><blockquote><blockquote></blockquote><p>Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]<br>Usage: spark-submit –kill [submission ID] –master [spark://…]<br>Usage: spark-submit –status [submission ID] –master [spark://…]<br>Usage: spark-submit run-example [options] example-class [example args]</p><blockquote></blockquote><p>Options: –master MASTER_URL spark://host:port, mesos://host:port, yarn, k8s://<a href="https://host:port/">https://host:port</a>, or</p><blockquote></blockquote><p>local (Default: local[*]). –deploy-mode DEPLOY_MODE 部署模式 client 或者 cluster 默认是client –class CLASS_NAME 运行java或者scala class(for Java / Scala apps). –name NAME 程序的名字 –jars JARS Comma-separated list of jars to include on the</p><blockquote></blockquote><p>driver and executor classpaths. –packages Comma-separated list of maven coordinates of</p><blockquote></blockquote><p>jars to include on the driver and executor classpaths. Will</p><blockquote></blockquote><p>search the local maven repo, then maven central and any</p><blockquote></blockquote><p>additional remote repositories given by –repositories. The</p><blockquote></blockquote><p>format for the coordinates should be</p><blockquote></blockquote><p>groupId:artifactId:version.<br>–exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in<br>– packages to avoid dependency conflicts. –repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with<br>– packages.<br>–py-files PY_FILES 指定Python程序依赖的其它python文件<br>–files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName).<br>–archives ARCHIVES Comma-separated list of archives to be extracted into the working directory of each executor.<br>–conf,<br>-c PROP=VALUE 手动指定配置<br>–properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark- defaults.conf. –driver-memory MEM Driver的可用内存(Default: 1024M). –driver-java-options Driver的一些Java选项 –driver-library-path Extra library path entries to pass to the driver. –driver-class-path Extra class path entries to pass to the driver. Note that jars added with –jars are automatically included in the classpath.<br>–executor-memory MEM Executor的内存 (Default: 1G).<br>–proxy-user NAME User to impersonate when submitting the application. This argument does not work with<br>–principal /<br>–keytab.<br>–help,<br>-h 显示帮助文件<br>–verbose,<br>-v Print additional debug output. –version, 打印版本 Cluster deploy mode only(集群模式专属):<br>–driver-cores NUM Driver可用的的CPU核数(Default: 1). Spark standalone or Mesos with cluster deploy mode only:<br>–supervise 如果给定, 可以尝试重启Driver Spark standalone, Mesos or K8s with cluster deploy mode only:<br>–kill SUBMISSION_ID 指定程序ID kill –status SUBMISSION_ID 指定程序ID 查看运行状态 Spark standalone, Mesos and Kubernetes only:<br>–total-executor-cores NUM 整个任务可以给Executor多少个CPU核心用 Spark standalone, YARN and Kubernetes only:<br>–executor-cores NUM 单个Executor能使用多少CPU核心 Spark on YARN and Kubernetes only(YARN模式下):<br>–num-executors NUM Executor应该开启几个<br>–principal PRINCIPAL Principal to be used to login to KDC.<br>–keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. Spark on YARN only:<br>–queue QUEUE_NAME 指定运行的YARN队列(Default: “default”)</p></blockquote></blockquote></li></ul><p>启动 YARN 的历史服务器</p><blockquote><p>cd /export/server/hadoop-3.3.0/sbin ./mr-jobhistory-daemon.sh start historyserver</p></blockquote><p>访问WebUI界面</p><blockquote><p><a href="http://master:19888/">http://master:19888/</a></p></blockquote><p>client 模式测试</p><blockquote><p>SPARK_HOME=/export/server/spark ${SPARK_HOME}/bin/spark-submit –master yarn –deploy-mode client – driver-memory 512m –executor-memory 512m –num-executors 1 –total- executor-cores 2 ${SPARK_HOME}/examples/src/main/python/pi.py</p></blockquote><p>cluster 模式测试</p><blockquote><p>SPARK_HOME=/export/server/spark ${SPARK_HOME}/bin/spark-submit –master yarn –deploy-mode cluster –driver- memory 512m –executor-memory 512m –num-executors 1 –total-executor-cores 2 –conf “spark.pyspark.driver.python=/root/anaconda3/bin/python3” –conf “spark.pyspark.python=/root/anaconda3/bin/python3” ${SPARK_HOME}/examples/src/main/python/pi.py 3**</p></blockquote><p>《Spark local&amp; stand-alone配置》</p><p>**</p><p>本地模式(单机) 本地模式就是以一个独立的进程,通过其内部的多个线程来模拟整个Spark运行时环境<br>Anaconda On Linux 安装 (单台服务器脚本安装)<br>安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装<br>位置在 /export/server:</p><blockquote><p>cd /export/server<br>#运行文件 sh Anaconda3-2021.05-Linux-x86_64.sh<br>过程显示：<br>…<br>#出现内容选 yes Please answer ‘yes’ or ‘no’:’ &gt;&gt;&gt; yes …<br>#出现添加路径：/export/server/anaconda3<br>…<br>[/root/anaconda3]</p><p>/export/server/anaconda3 PREFIX=/export/server/anaconda3<br>…</p></blockquote><p>安装完成后, 退出终端， 重新进来:</p><blockquote><p>exit<br>结果显示：<br>#看到这个Base开头表明安装好了.base是默认的虚拟环境. Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1 (base)<br>[root@node1 ~]#</p></blockquote><p>创建虚拟环境 pyspark 基于 python3.8</p><blockquote><p>conda create -n pyspark python=3.8</p></blockquote><p>切换到虚拟环境内</p><blockquote><p>conda activate pyspark 结果显示： (base) [root@node1 ~]# conda activate pyspark (pyspark) [root@node1 ~]#</p></blockquote><p>在虚拟环境内安装包 （有WARNING不用管）</p><blockquote><p>pip install pyhive pyspark jieba -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></p></blockquote><p>spark 安装<br>将文件上传到 /export/server 里面 ，解压</p><blockquote><p>cd /export/server # 解压 tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/</p></blockquote><p>建立软连接</p><blockquote><p>ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</p></blockquote><p>添加环境变量</p><blockquote><p>SPARK_HOME: 表示Spark安装路径在哪里<br>PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器<br>JAVA_HOME: 告知Spark Java在哪里<br>HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里<br>HADOOP_HOME: 告知Spark Hadoop安装在哪里</p></blockquote><p>vim /etc/profile<br>内容：<br>…..<br>注：此部分之前配置过，此部分不需要在配置<br>#JAVA_HOME export JAVA_HOME=/export/server/jdk1.8.0_241 export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</p><p>#HADOOP_HOME export HADOOP_HOME=/export/server/hadoop-3.3.0 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</p><p>#ZOOKEEPER_HOME export ZOOKEEPER_HOME=/export/server/zookeeper export PATH=$PATH:$ZOOKEEPER_HOME/bin …..</p><p>#将以下部分添加进去 #SPARK_HOME export SPARK_HOME=/export/server/spark #HADOOP_CONF_DIR export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop #PYSPARK_PYTHON export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python<br>vim .bashrc<br>内容添加进去：</p><p>#JAVA_HOME<br>export JAVA_HOME=/export/server/jdk1.8.0_241<br>#PYSPARK_PYTHON<br>export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</p><p>重新加载环境变量文件</p><blockquote><p>source /etc/profile<br>source ~/.bashrc</p></blockquote><p>进入 /export/server/anaconda3/envs/pyspark/bin/ 文件夹</p><blockquote><p>cd /export/server/anaconda3/envs/pyspark/bin/</p></blockquote><p>开启</p><blockquote><p>./pyspark 结果显示： (base) [root@master bin]# ./pyspark<br>Python 3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0] :: Anaconda, Inc. on linux<br>Type “help”, “copyright”, “credits” or “license” for more information.<br>Setting default log level to “WARN”. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 2022-03-15 20:37:04,612 WARN util.NativeCodeLoader: Unable to load native- hadoop library for your platform… using builtin-java classes where applicable<br>Welcome to</p><hr><p>__ / <strong>/</strong> ___ <em>****/ /**<br>*\ / _ / _ `/ <em>*/ ‘*/<br>/</em>* / .**/</em>,*/<em>/ /</em>/_\ version 3.2.0<br>/<em>/<br>Using Python version 3.8.12 (default, Oct 12 2021 13:49:34) Spark context Web UI available at <a href="http://master:4040/">http://master:4040</a> Spark context available as ‘sc’ (master = local[</em>], app id = local- 1647347826262). SparkSession available as ‘spark’. &gt;&gt;&gt;</p></blockquote><p>查看WebUI界面</p><blockquote><p>浏览器访问：<br><a href="http://node1:4040/">http://node1:4040/</a></p></blockquote><p>退出</p><blockquote><p>conda deactivate</p></blockquote><p>Standalone模式(集群) Spark中的各个角色以独立进程的形式存在,并组成Spark集群环境 Anaconda On Linux 安装 (单台服务器脚本安装 注：在 slave1 和 slave2 上部署)<br>安装上传安装包: 资料中提供的Anaconda3-2021.05-Linux-x86_64.sh文件到Linux服务器上安装位置在 /export/server:</p><p>cd /export/server # 运行文件 sh Anaconda3-2021.05-Linux-x86_64.sh</p><blockquote><p>过程显示：<br>…<br>#出现内容选 yes<br>Please answer ‘yes’ or ‘no’:’<br>yes<br>…<br>#出现添加路径：/export/server/anaconda3<br>…<br>[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3 PREFIX=/export/server/anaconda3<br>…</p></blockquote><p>安装完成后, 退出终端，</p><blockquote><p>重新进来:<br>exit<br>结果显示：<br>#看到这个Base开头表明安装好了.base是默认的虚拟环境.<br>Last login: Tue Mar 15 15:28:59 2022 from 192.168.88.1<br>…</p></blockquote><p>在 master 节点上把 ./bashrc 和 profile 分发给 slave1 和 slave2</p><blockquote><p>#分发 .bashrc : scp <del>/.bashrc root@slave1:</del>/ scp <del>/.bashrc root@slave2:</del>/ #分发 profile : scp /etc/profile/ root@slave1:/etc/ scp /etc/profile/ root@slave2:/etc/<br>…</p></blockquote><p>创建虚拟环境 pyspark 基于 python3.8</p><blockquote><p>conda create -n pyspark python=3.8</p></blockquote><p>切换到虚拟环境内</p><blockquote><p>conda activate pyspark 结果显示： (base) [root@node1 ~]# conda activate pyspark (pyspark)<br>在虚拟环境内安装包 （有WARNING不用管）<br>pip install pyhive pyspark jieba -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a><br>spark 安装<br>将文件上传到 /export/server 里面 ，解压</p></blockquote><p>master 节点节点进入 /export/server/spark/conf 修改以下配置文件</p><blockquote><p>cd /export/server/spark/conf</p></blockquote><p>将文件 workers.template 改名为 workers，并配置文件内容</p><blockquote><p>mv workers.template workers vim workers<br># localhost删除，内容追加文末： node1<br>node2<br>node3<br># 功能: 这个文件就是指示了 当前SparkStandAlone环境下, 有哪些worker</p></blockquote><p>将文件 spark-env.sh.template 改名为 spark-env.sh，并配置相关内容</p><blockquote><p>mv spark-env.sh.template spark-env.sh vim spark-env.sh</p></blockquote><blockquote><p>文末追加内容：<br>##设置JAVA安装目录 JAVA_HOME=/export/server/jdk<br>##HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群 HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop YARN_CONF_DIR=/export/server/hadoop/etc/hadoop ## 指定spark老大Master的IP和提交任务的通信端口 # 告知Spark的master运行在哪个机器上 export SPARK_MASTER_HOST=master<br>#告知sparkmaster的通讯端口 export SPARK_MASTER_PORT=7077<br># 告知spark master的 webui端口 SPARK_MASTER_WEBUI_PORT=8080<br># worker cpu可用核数 SPARK_WORKER_CORES=1<br># worker可用内存 SPARK_WORKER_MEMORY=1g<br># worker的工作通讯地址 SPARK_WORKER_PORT=7078<br># worker的 webui地址 SPARK_WORKER_WEBUI_PORT=8081<br>## 设置历史服务器 # 配置的意思是 将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中 SPARK_HISTORY_OPTS=”- Dspark.history.fs.logDirectory=hdfs://master:8020/sparklog/ - Dspark.history.fs.cleaner.enabled=true”</p></blockquote><p>开启 hadoop 的 hdfs 和 yarn 集群</p><blockquote><blockquote><pre class="line-numbers language-none"><code class="language-none">start-dfs.sh start-yarn.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>在HDFS上创建程序运行历史记录存放的文件夹，同样 conf 文件目录下:</p><p>hadoop fs -mkdir /sparklog hadoop fs -chmod 777 /sparklog</p></blockquote></blockquote><p>将 spark-defaults.conf.template 改为 spark-defaults.conf 并做相关配置</p><blockquote><p>mv spark-defaults.conf.template spark-defaults.conf vim spark-defaults.conf 文末追加内容为： # 开启spark的日期记录功能 spark.eventLog.enabled true # 设置spark日志记录的路径 spark.eventLog.dir hdfs://master:8020/sparklog/ # 设置spark日志是否启动压缩 spark.eventLog.compress true</p></blockquote><p>配置 log4j.properties 文件 将文件第 19 行的 log4j.rootCategory=INFO, console 改为<br>log4j.rootCategory=WARN, console （即将INFO 改为 WARN 目的：输出日志, 设置级别为<br>WARN 只输出警告和错误日志，INFO 则为输出所有信息，多数为无用信息）</p><blockquote><p>mv log4j.properties.template log4j.properties vim log4j.properties 结果显示：<br>…<br>18 # Set everything to be logged to the console<br>19 log4j.rootCategory=WARN, console ….</p></blockquote><p>master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上</p><blockquote><p>master 节点分发 spark 安装文件夹 到 slave1 和 slave2 上</p></blockquote><p>在slave1 和 slave2 上做软连接</p><blockquote><p>ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</p></blockquote><p>重新加载环境变量</p><blockquote><p>source /etc/profile</p></blockquote><p>进入 /export/server/spark/sbin 文件目录下 启动 start-history-server.sh</p><blockquote><p>cd /export/server/spark/sbin ./start-history-server.sh</p></blockquote><p>访问 WebUI 界面</p><blockquote><p>浏览器访问： <a href="http://master:18080/">http://master:18080/</a></p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>《spark基础环境配置》</title>
      <link href="/2022/05/31/spark1/"/>
      <url>/2022/05/31/spark1/</url>
      
        <content type="html"><![CDATA[<p>打开一个hosts映射文件,为了保证后续相互关联的虚拟机能够通过主机名进行访问，根据实际需求配置<br>对应的IP和主机名映射，分别将主机名master、slave1、slave2 与IP地址 192.168.88.134、<br>192.168.88.135 和192.168.88.136进行了匹配映射(这里通常要根据实际需要，将要搭建的集群主机都<br>配置主机名和IP映射)。<br>编辑 /etc/hosts 文件</p><blockquote><p>vim /etc/hosts<br>内容修改为（注：三台主机内容一样）<br>localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6<br>192.168.88.135 node1<br>192.168.88.136 node2<br>192.168.88.137 node3</p></blockquote><p>三、集群配置时间同步<br>定义：网络时间服务协议（Network Time Protocol, NTP），是用来使计算机时间同步化的一种协议，它可以使计算机对其服务器做时间同步化。<br>原因：时间同步服务器，顾名思义就是来同步时间的。在集群中同步时间有着十分重要的作用，负载均衡集群或高可用集群如果时间不一致，在服务器之间的数据误差就会很大，寻找数据便会成为一件棘手的事情。若是时间无法同步，那么就算是备份了数据，你也可能无法在正确的时间将正确的数据备份。那损失可就大了。<br>yum 安装 ntp （注：三台主机做同样操作）</p><blockquote><p>yum install ntp -y<br>开机自启动ntp<br>systemctl enable ntpd &amp;&amp; systemctl start ntpd<br>结果显示： [root@master ~]# systemctl enable ntpd &amp;&amp; systemctl start ntpd Created symlink from /etc/systemd/system/multi- user.target.wants/ntpd.service to /usr/lib/systemd/system/ntpd.service.</p></blockquote><p>授权 192.168.88.0-192.168.10.255 网段上的所有机器可以从这台机器上查询和同步时间</p><blockquote><p>#查看ntp配置文件<br>ls -al /etc | grep ‘ntp’<br>#显示内容<br>[root@node1 etc]# ls -al /etc | grep ‘ntp’<br>drwxr-xr-x 3 root root 52 3月 10 18:25 ntp<br>-rw-r–r– 1 root root 2041 3月 10 20:03 ntp.conf<br>#编辑内容添加 restrict 192.168.88.0 mask 255.255.255.0 （注：在17行左右） vim /etc/ntp.conf<br>16 # Hosts on local network are less restricted.<br>17 restrict 192.168.88.0 mask 255.255.255.0</p></blockquote><p>集群在局域网中，不使用其他互联网上的时间</p><blockquote><blockquote><pre class="line-numbers language-none"><code class="language-none">#修改 /etc/ntpd.conf 内容vim vim /etc/ntp.conf # <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></blockquote></blockquote><pre class="line-numbers language-none"><code class="language-none">将21-24行内容注释掉（注：原来未注释）   21 #server 0.centos.pool.ntp.org iburst 22 #server 1.centos.pool.ntp.org iburst 23 #server 2.centos.pool.ntp.org iburst 24 #server 3.centos.pool.ntp.org iburst # 在25行添加 server masterIP 即为： server 192.168.88.135<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>node 和 node3 相同操作<br>三台主机同时执行</p><blockquote><p>systemctl enable ntpd &amp;&amp; systemctl start ntpd</p></blockquote><p>查看ntp端口</p><blockquote><p>[root@master etc]# ss -tupln | grep ‘123’ udp UNCONN 0 0 192.168.88.135:123 <em>:</em> users:((“ntpd”,pid=54823,fd=19)) udp UNCONN 0 0 127.0.0.1:123 <em>:</em> users:((“ntpd”,pid=54823,fd=18)) udp UNCONN 0 0 <em>:123 *:</em> users:((“ntpd”,pid=54823,fd=16)) udp UNCONN 0 0 [fe80::2832:5f98:5bc0:e621]%ens33:123 [::]:* users:((“ntpd”,pid=54823,fd=23)) udp UNCONN 0 0 [::1]:123 [::]:* users:((“ntpd”,pid=54823,fd=20)) udp UNCONN 0 0 [::]:123 [::]:* users:((“ntpd”,pid=54823,fd=17))</p></blockquote><p>配置完成后三台主机都需要重启</p><blockquote><p>shutdown -r 0</p></blockquote><p>三台主机同时执行（注：此过程需要5分钟左右）</p><blockquote><p>ntpstat</p></blockquote><p>三、ssh免密钥登陆<br>SSH免密钥登陆可以更加方便的实现不同计算机之间的连接和切换<br>master 生成公钥私钥 (一路回车)<br>ssh-keygen</p><blockquote><p>#结果显示：<br>[root@master .ssh]# ssh-keygen Generating public/private rsa key pair.<br>Enter file in which to save the key (/root/.ssh/id_rsa):<br>Enter passphrase (empty for no passphrase):<br>Enter same passphrase again:<br>Your identification has been saved in /root/.ssh/id_rsa.<br>Your public key has been saved in /root/.ssh/id_rsa.pub. T<br>he key fingerprint is: SHA256:QUAgFH5KBc/Erlf1JWSBbKeEepPJqMBqpWbc02/uFj8 root@master The key’s randomart image is:<br>+—[RSA 2048]—-+<br>| .=++oo+.o+. |<br>| . <em>. ..</em>.o . |<br>|. o.++ *.+ o |</p></blockquote><pre class="line-numbers language-none"><code class="language-none">|.o ++ B ...      | |o.=o.o .S        ||.*oo.. .         | |+ .. . o         | | + E             | | =o .            | +----[SHA256]-----+<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>查看隐藏的 .ssh 文件</p><blockquote><p>la -al .ssh</p></blockquote><pre class="line-numbers language-none"><code class="language-none"># 结果显示 [root@master ~]# ls -al .ssh/ 总用量 16 drwx------ 2 root root 80 3月 10 21:52 . dr-xr-x---. 4 root root 175 3月 10 21:45 .. -rw------- 1 root root 393 3月 10 21:52 authorized_keys -rw------- 1 root root 1675 3月 10 21:48 id_rsa -rw-r--r-- 1 root root 393 3月 10 21:48 id_rsa.pub -rw-r--r-- 1 root root 366 3月 10 21:54 known_hosts<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>master 配置免密登录到master slave1 slave2</p><blockquote><p>ssh-copy-id master<br>ssh-copy-id slave1<br>ssh-copy-id slave2</p></blockquote><p>四、安装配置 jdk<br>编译环境软件安装目录</p><blockquote><p>mkdir -p /export/server</p></blockquote><p>JDK 1.8安装 上传 jdk-8u241-linux-x64.tar.gz到/export/server/目录下 并解压文件</p><blockquote><p>tar -zxvf jdk-8u241-linux-x64.tar.gz</p></blockquote><p>配置环境变量</p><blockquote><p>vim /etc/profile export JAVA_HOME=/export/server/jdk1.8.0_241 export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</p></blockquote><p>重新加载环境变量文件</p><blockquote><p>source /etc/profile</p></blockquote><p>查看 java 版本号</p><blockquote><p>java -version 结果显示： [root@master jdk1.8.0_241]# java -version java version “1.8.0_241” Java(TM) SE Runtime Environment (build 1.8.0_241-b07) Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode)</p></blockquote><p>master 节点将 java 传输到 slave1 和 slave2</p><blockquote><p>scp -r /export/server/jdk1.8.0_241/ root@slave1:/export/server/ scp -r /export/server/jdk1.8.0_241/ root@slave2:/export/server/</p></blockquote><p>配置 slave1 和 slave2 的 jdk 环境变量（注：和上方 master 的配置方法一样）<br>在 master slave1 和slave2 创建软连接</p><blockquote><p>cd /export/server<br>ln -s jdk1.8.0_241/ jdk</p></blockquote><p>重新加载环境变量文件</p><blockquote><p>source /etc/profile</p></blockquote><p>zookeeper安装配置<br>配置主机名和IP的映射关系，修改 /etc/hosts 文件，添加 master.root slave1.root slave2.root</p><blockquote><blockquote><pre class="line-numbers language-none"><code class="language-none">vim /etc/hosts #结果显示 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></blockquote></blockquote><pre class="line-numbers language-none"><code class="language-none">192.168.88.135 master master.root 192.168.88.136 slave1 slave1.root 192.168.88.137 slave2 slave2.root<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>zookeeper安装 上传 zookeeper-3.4.10.tar.gz到/export/server/目录下 并解压文件</p><blockquote><p>cd /export/server/ tar -zxvf zookeeper-3.4.10.tar.gz</p></blockquote><p>在 /export/server 目录下创建软连接</p><blockquote><p>cd /export/server ln -s zookeeper-3.4.10/ zookeeper</p></blockquote><p>进入 /export/server/zookeeper/conf/ 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg</p><blockquote><p>cd /export/server/zookeeper/conf/ cp zoo_sample.cfg zoo.cfg</p></blockquote><p>接上步给 zoo.cfg 添加内容</p><blockquote><p>#Zookeeper的数据存放目录 dataDir=/export/server/zookeeper/zkdatas<br># 保留多少个快照 autopurge.snapRetainCount=3<br># 日志多少小时清理一次 autopurge.purgeInterval=1<br># 集群中服务器地址 server.1=master:2888:3888 server.2=slave1:2888:3888 server.3=slave2:2888:3888</p></blockquote><p>进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去</p><blockquote><p>cd /export/server/zookeeper/zkdata touch myid echo ‘1’ &gt; myid</p></blockquote><p>将 master 节点中 /export/server/zookeeper-3.4.10 路径下内容推送给slave1 和 slave2</p><blockquote><p>scp -r /export/server/zookeeper-3.4.10/ slave1:$PWD scp -r /export/server/zookeeper-3.4.10/ slave2:$PWD</p></blockquote><p>推送成功后，分别在 slave1 和 slave2 上创建软连接</p><blockquote><p>ln -s zookeeper-3.4.10/ zookeeper</p></blockquote><p>接上步推送完成后将 slave1 和 slave2 的 /export/server/zookeeper/zkdatas/ 文件夹下的 myid中的内容分别改为 2 和3</p><blockquote><p>cd /export/server/zookeeper/zkdatas/ 结果显示： [root@slave1 zkdatas]# vim myid [root@slave1 zkdatas]# more myid 2[root@slave2 zkdatas]# vim myid [root@slave2 zkdatas]# more myid 3</p></blockquote><p>配置zookeeper的环境变量（注：三台主机都需要配置）</p><blockquote><p>vim /etc/profile # zookeeper 环境变量 export ZOOKEEPER_HOME=/export/server/zookeeper export PATH=$PATH:$ZOOKEEPER_HOME/bin</p></blockquote><p>重新加载环境变量文件</p><blockquote><p>source /etc/profile</p></blockquote><p>进入 /export/server/zookeeper-3.4.10/bin 目录下启动 zkServer.sh 脚本 （注：三台都需要做）</p><blockquote><p>cd /export/server/zookeeper-3.4.10/bin zkServer.sh start<br>结果显示： [root@master bin]# ./zkServer.sh start ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Starting zookeeper … STARTED</p></blockquote><p>zookeeper 的状态</p><blockquote><blockquote><pre class="line-numbers language-none"><code class="language-none">zkServer.sh status 结果显示： [root@master server]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@slave1 server]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: follower [root@slave2 conf]# zkServer.sh status ZooKeeper JMX enabled by default Using config: /export/server/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: leader<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></blockquote></blockquote><p>jps 结果显示：</p><blockquote><p>[root@master server]# jps 125348 QuorumPeerMain 16311 Jps [root@slave1 server]# jps 126688 QuorumPeerMain 17685 Jps [root@slave2 conf]# jps 126733 QuorumPeerMain 17727 Jps</p></blockquote><p>脚本一键启动</p><blockquote><p>vim zkServer.sh<br>#!/bin/bash<br>if [ $# -eq 0 ] ;<br>then<br>echo “please input param:start stop”<br>else<br>if [ $1 = start ] ;then<br>echo “${1}ing master”<br>ssh master “source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start”<br>for i in {1..2}<br>do<br>echo “${1}ping slave${i}”</p></blockquote><pre class="line-numbers language-none"><code class="language-none">     ssh slave${i} "source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start" done <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><blockquote><p>fi<br>if [ $1 = stop ];then<br>echo “${1}ping master “<br>ssh master “source /etc/profile;/export/server/zookeeper/bin/zkServer.sh stop”<br>for i in {1..2}<br>do<br>echo “${1}ping slave${i}” ssh slave${i} “source /etc/profile;/export/server/zookeeper/bin/zkServer.sh stop”<br>done<br>fi<br>if [ $1 = status ];then<br>echo “${1}ing master”<br>ssh master “source /etc/profile;/export/server/zookeeper/bin/zkServer.sh status”<br>for i in {1..2}<br>do<br>echo “${1}ping slave${i}”<br>ssh slave${i} “source /etc/profile;<br>/export/server/zookeeper/bin/zkServer.sh status”<br>done<br>fi<br>fi<br>#将文件放在 /bin 目录下 chmod +x zkServer-all.sh &amp;&amp; zkServer-all.sh</p></blockquote><p>Hadoop 安装配置<br>把 hadoop-3.3.0-Centos7-64-with-snappy.tar.gz 上传到 /export/server 并解压文件</p><blockquote><p>tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz</p></blockquote><pre class="line-numbers language-none"><code class="language-none">修改配置文件(进入路径 /export/server/hadoop-3.3.0/etc/hadoop)cd /export/server/hadoop-3.3.0/etc/hadoophadoop-env.sh#文件最后添加 export JAVA_HOME=/export/server/jdk1.8.0_241 export HDFS_NAMENODE_USER=root export HDFS_DATANODE_USER=root export HDFS_SECONDARYNAMENODE_USER=root export YARN_RESOURCEMANAGER_USER=root export YARN_NODEMANAGER_USER=rootcore-site.xml&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 - -&gt;&lt;property&gt;          &lt;name&gt;fs.defaultFS&lt;/name&gt;          &lt;value&gt;hdfs://master:8020&lt;/value&gt;          &lt;/property&gt; &lt;!-- 设置Hadoop本地保存数据路径 --&gt; &lt;property&gt;     &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;     &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt; &lt;/property&gt; &lt;!-- 设置HDFS web UI用户身份 --&gt; &lt;property&gt;      &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;      &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>hadoop.proxyuser.root.hosts * </p><p>hadoop.proxyuser.root.groups * </p><p>&lt; !– 文件系统垃圾桶保存时间 –&gt; </p><p>fs.trash.interval 1440 </p><p>hdfs-site.xml </p><p>dfs.namenode.secondary.http-address slave1:9868 </p><p>mapred-site.xml </p><p>mapreduce.framework.name yarn </p><p>mapreduce.jobhistory.address master:10020 </p><p>mapreduce.jobhistory.webapp.address master:19888 </p><p>yarn.app.mapreduce.am.env HADOOP_MAPRED_HOME=${HADOOP_HOME} </p><p>mapreduce.map.env HADOOP_MAPRED_HOME=${HADOOP_HOME} </p><p>mapreduce.reduce.env HADOOP_MAPRED_HOME=${HADOOP_HOME} </p><p>yarn-site.xml </p><p>yarn.resourcemanager.hostname master </p><p>yarn.nodemanager.aux-services mapreduce_shuffle </p><p>yarn.nodemanager.pmem-check-enabled false </p><p>yarn.nodemanager.vmem-check-enabled false </p><p>yarn.log-aggregation-enable true </p><p>yarn.log.server.url <a href="http://master:19888/jobhistory/logs">http://master:19888/jobhistory/logs</a> </p><p>yarn.log-aggregation.retain-seconds 604800</p></blockquote><pre class="line-numbers language-none"><code class="language-none">node1node2node3<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>分发同步hadoop安装包</p><blockquote><p>cd /export/server<br>scp -r hadoop-3.3.0 root@slave1:$PWD<br>scp -r hadoop-3.3.0 root@slave2:$PWD<br>将hadoop添加到环境变量</p><p>vim /etc/profile export HADOOP_HOME=/export/server/hadoop-3.3.0 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</p></blockquote><p>重新加载环境变量文件</p><blockquote><p>source /etc/profile</p></blockquote><pre class="line-numbers language-none"><code class="language-none">Hadoop集群启动格式化namenode（只有首次启动需要格式化）hdfs namenode -format<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>脚本一键启动</p><blockquote><p>[root@master ~]# start-dfs.sh Starting namenodes on [master] 上一次登录：五 3月 11 21:27:24 CST 2022pts/0 上 Starting datanodes 上一次登录：五 3月 11 21:27:32 CST 2022pts/0 上 Starting secondary namenodes [slave1] 上一次登录：五 3月 11 21:27:35 CST 2022pts/0 上</p></blockquote><pre class="line-numbers language-none"><code class="language-none">[root@master ~]# start-yarn.sh Starting resourcemanager 上一次登录：五 3月 11 21:27:41 CST 2022pts/0 上 Starting nodemanagers 上一次登录：五 3月 11 21:27:51 CST 2022pts/0 上启动后 输入 jps 查看[root@master ~]# jps 127729 NameNode 127937 DataNode 14105 Jps 128812 NodeManager 128591 ResourceManager [root@slave1 hadoop]# jps 121889 NodeManager 121559 SecondaryNameNode 7014 Jps 121369 DataNode [root@slave2 hadoop]# jps 6673 Jps 121543 NodeManager 121098 DataNode<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>WEB页面<br>HDFS集群：</p><blockquote><blockquote><pre class="line-numbers language-none"><code class="language-none">http://master:9870/<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></blockquote></blockquote><p>YARN集群：</p><blockquote><p><a href="http://master:9870/">http://master:9870/</a></p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/05/24/hello-world/"/>
      <url>/2022/05/24/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
